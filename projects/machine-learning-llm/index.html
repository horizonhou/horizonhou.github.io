<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Predicting Box Office Failures with LLM | Ruize Hou</title><meta name=keywords content><meta name=description content="Abstract This report presents a comprehensive data science study designed to predict box office performance of movies, leveraging an integrated analysis of three extensive datasets: MovieLens 25M, IMDb Non-commericial, and Kaggle Movie Revenue. These datasets collectively provide a wealth of movie features, including ratings, revenue, metadata, and audience preferences. The study primarily aims to identify the key variables that influence a movie&rsquo;s box office success or failure, seeking to unravel the potential impact of a movie&rsquo;s budget on its box office performance, the likelihood of certain genres failing at the box office, and the strategies that can be optimized to maximize returns on movie production investments."><meta name=author content="Ruize Hou, Hongjiao Zhang, Jiao Wang, Zijin Gao, Curtis Chen"><link rel=canonical href=https://horizonhou.github.io/projects/machine-learning-llm/><link crossorigin=anonymous href=/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://horizonhou.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://horizonhou.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://horizonhou.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://horizonhou.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://horizonhou.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Predicting Box Office Failures with LLM"><meta property="og:description" content="Abstract This report presents a comprehensive data science study designed to predict box office performance of movies, leveraging an integrated analysis of three extensive datasets: MovieLens 25M, IMDb Non-commericial, and Kaggle Movie Revenue. These datasets collectively provide a wealth of movie features, including ratings, revenue, metadata, and audience preferences. The study primarily aims to identify the key variables that influence a movie&rsquo;s box office success or failure, seeking to unravel the potential impact of a movie&rsquo;s budget on its box office performance, the likelihood of certain genres failing at the box office, and the strategies that can be optimized to maximize returns on movie production investments."><meta property="og:type" content="article"><meta property="og:url" content="https://horizonhou.github.io/projects/machine-learning-llm/"><meta property="og:image" content="https://horizonhou.github.io/projects/machine-learning-llm/covers/BoxOffice.jpg"><meta property="article:section" content="projects"><meta property="article:published_time" content="2023-05-26T23:15:00+07:00"><meta property="article:modified_time" content="2023-05-26T23:15:00+07:00"><meta property="og:site_name" content="RuizeHou"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://horizonhou.github.io/projects/machine-learning-llm/covers/BoxOffice.jpg"><meta name=twitter:title content="Predicting Box Office Failures with LLM"><meta name=twitter:description content="Abstract This report presents a comprehensive data science study designed to predict box office performance of movies, leveraging an integrated analysis of three extensive datasets: MovieLens 25M, IMDb Non-commericial, and Kaggle Movie Revenue. These datasets collectively provide a wealth of movie features, including ratings, revenue, metadata, and audience preferences. The study primarily aims to identify the key variables that influence a movie&rsquo;s box office success or failure, seeking to unravel the potential impact of a movie&rsquo;s budget on its box office performance, the likelihood of certain genres failing at the box office, and the strategies that can be optimized to maximize returns on movie production investments."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"https://horizonhou.github.io/projects/"},{"@type":"ListItem","position":2,"name":"Predicting Box Office Failures with LLM","item":"https://horizonhou.github.io/projects/machine-learning-llm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Predicting Box Office Failures with LLM","name":"Predicting Box Office Failures with LLM","description":"Abstract This report presents a comprehensive data science study designed to predict box office performance of movies, leveraging an integrated analysis of three extensive datasets: MovieLens 25M, IMDb Non-commericial, and Kaggle Movie Revenue. These datasets collectively provide a wealth of movie features, including ratings, revenue, metadata, and audience preferences. The study primarily aims to identify the key variables that influence a movie\u0026rsquo;s box office success or failure, seeking to unravel the potential impact of a movie\u0026rsquo;s budget on its box office performance, the likelihood of certain genres failing at the box office, and the strategies that can be optimized to maximize returns on movie production investments.","keywords":[],"articleBody":" Abstract This report presents a comprehensive data science study designed to predict box office performance of movies, leveraging an integrated analysis of three extensive datasets: MovieLens 25M, IMDb Non-commericial, and Kaggle Movie Revenue. These datasets collectively provide a wealth of movie features, including ratings, revenue, metadata, and audience preferences. The study primarily aims to identify the key variables that influence a movie’s box office success or failure, seeking to unravel the potential impact of a movie’s budget on its box office performance, the likelihood of certain genres failing at the box office, and the strategies that can be optimized to maximize returns on movie production investments. We also delve into the role of marketing expenses in shaping a movie’s box office performance, providing actionable insights that can help refine and enhance the effectiveness of marketing campaigns. The datasets used in this project are extensive, with the MovieLens dataset encompassing 25 million ratings applied to 62,000 movies by 162,000 users, the IMDb dataset providing a vast trove of movie data, and the Kaggle Revenue dataset offering 26 million ratings from 270,000 users for 45,000 movies. Rigorous data cleaning techniques were applied to ensure the integrity and accuracy of the datasets. For our prediction models, we utilized a range of machine learning algorithms, including Random Forest, Logistic Regression, XGBoost, and Gradient Boosted Trees, achieving a testing accuracy of around 0.92. The implications of this research are substantial for the film industry. By offering predictive models and insights that can help mitigate the risk of box office failure, optimize both production and marketing strategies, and enhance the return on investment for movie projects, this study contributes significantly to understanding audience preferences and establishing a pathway towards increased profitability in the movie industry.\nIntroduction In an industry where the thin line between success and failure often lies in the delicate balance of creativity, budgeting, and marketing, understanding the dynamics of box office performance is crucial. As a group of students with a shared passion for film and movies, we were driven to use our data science skills to delve into the intriguing complexities of the film industry. The motivation behind our study was not just academic curiosity; we sought to contribute tangible insights that could inform and influence the industry’s approach to production and investment strategies.\nOur project aims to predict box office success by analyzing an extensive array of movie features and audience preferences. To achieve this, we have combined three expansive datasets: MovieLens, IMDb, and Kaggle Revenue. These datasets collectively provide a wealth of data, including ratings, revenue, and metadata for thousands of movies. Specifically, the MovieLens dataset comprises 25 million ratings applied to 62,000 movies by 162,000 users, the IMDb dataset offers a comprehensive array of movie data, and the Kaggle Revenue dataset presents 26 million ratings from 270,000 users for 45,000 movies.\nThe scope of our study involves identifying the key variables that influence a movie’s box office performance. We intend to unravel questions like the impact of a movie’s budget on its box office performance, the susceptibility of certain genres to failure, and the role of marketing expenses in shaping a movie’s financial success. Our goal is not only to provide predictive models for box office success but also to generate insights that can help optimize production and marketing strategies.\nBy merging our passion for the film industry with our expertise in data science, we aim to shed light on the factors that contribute to a movie’s success or failure at the box office. We believe that our research will serve as a valuable resource for film studios and investors, providing them with data-driven insights to optimize their decisions and investments in the movie industry.\nRelated Work In this section, we summarize some work that is most relevant to ours.\nOur study builds upon a rich body of academic research that has applied data science to the film industry. A seminal resource for our work was the MovieLens dataset, the creation and application of which was discussed in Harper and Konstan’s \"The MovieLens Datasets: History and Context\" [@harper2015movielens]. This dataset, encompassing 25 million ratings applied to 62,000 movies, has proven invaluable in numerous studies for understanding audience preferences and movie ratings.\nSimultaneously, sentiment analysis has emerged as a significant tool in this field. Asano and Motomura’s \"Large-Scale Movie Review Dataset for Sentiment Analysis\" [@asano2015large] demonstrates the utility of sentiment analysis in understanding public opinion about movies, an approach that complements traditional numerical ratings.\nThe question of whether critical reviews influence box office success has also been examined. Maltby’s study \"Predicting box-office success: do critical reviews really matter?\" [@maltby2000predicting] provided insights into the interplay between critical opinions and audience reception, an important factor in determining a movie’s financial performance.\nIn recent years, machine learning has been increasingly leveraged to predict box office performance. Narayanan et al.’s study [@narayanan2019box] and Subramaniyaswamy et al.’s research [@2017subramaniyaswamy] both employed machine learning algorithms to predict box office success, laying the groundwork for our own predictive models. These models have been further refined by studies such as \"A Machine Learning Approach to Predict Movie Revenue Based on Pre-Released Movie Metadata\" [@ML2020] and \"Finding Nemo: Predicting Movie Performances by Machine Learning Methods\" [@Nemo2020], which emphasize the predictive power of pre-released movie metadata.\nKim et al.’s study [@SNS2015] and Shen’s research [@Shen2020] took a different approach by integrating social media data into their machine learning models. They demonstrated that the sentiment expressed on social media platforms could be a valuable predictor of box office performance, a finding that underscores the importance of considering a wide range of data sources in our study.\nOur research aligns with and extends upon a significant body of existing academic work. We aspire to contribute to this ongoing dialogue by providing new insights into the predictors of box office success and exploring the potential for data science to optimize film industry practices.\nDataset Our study’s dataset is a robust combination of three widely-recognized sources, namely the MovieLens dataset, the IMDb dataset, and the Revenue dataset from Kaggle. These datasets collectively present a comprehensive array of movie features essential for our research.\nThe MovieLens dataset, developed by the GroupLens research lab at the University of Minnesota, is an extensive resource that contains 25 million ratings and one million tag applications. These ratings and tags have been applied to a total of 62,000 movies by an active user base of 162,000 individuals. This dataset allows us to access a vast and diverse range of user opinions on a large collection of movies.\nIMDb is a renowned online database that provides a plethora of information about films, television programs, and video games, including cast, production crew, plot summaries, and ratings. This dataset provides us with a broad range of movie metadata, offering deeper insights into the features that might influence a movie’s box office performance.\nLastly, the Revenue dataset from Kaggle includes 26 million ratings from 270,000 users for 45,000 movies, is a rich source of revenue data. It allows us to study the relationship between various movie features and their box office performance.\nThe combined dataset presents a comprehensive collection of movie features, ratings, revenue data, and other associated metadata. The breadth and depth of these datasets provide a strong foundation for our analysis and predictive modeling in this study.\nData Preprocessing Our data-driven exploration began with a comprehensive process of preparing and understanding our data. Exploratory Data Analysis (EDA) was a critical part of this process. We examined our data using various statistical and visualization tools to understand the features and relationships within our dataset. Our exploration included the creation of histograms to understand the distribution of our data, boxplots to identify outliers, and a coefficient matrix to examine the relationships between variables.\nThrough outlier analysis, we found that there exists many outliers in budget and revenue. There are many successful movies that have enormous investment in production and outstanding performance. However, we didn’t want to remove these outliers because we want to take all types of movie, no matter it is big production or not, into consideration. We will mitigate the outlier effect by making our target variable binary and also through mitigating the imbalance of the data.\nThe insights from our EDA guided our data cleaning process. We identified and addressed anomalies such as duplicate entries and missing data. We also did our best to fill in gaps in our dataset where the data was numerical. For example, there were missing ‘budget’ values, which we estimated using other features like ‘year’ and ‘genre’, or using online resources to impute missing values. Figure [4] is the data without preprocessing and figure [3] is the data after removing entries with missing budgets, which are entries with budget of 0 in the data.\nWe also acknowledged the potential for bias in our dataset. The IMDb dataset may favor specific movie types or genres, and this had to be addressed to ensure fair analysis. We mitigated this by integrating box office revenue data into our analysis to complement the IMDb data. Resolving class imbalance can begin with resampling techniques. This process involves adjusting the dataset by either oversampling the minority class, which duplicates instances from the minority class, or undersampling the majority class by removing instances. The goal is to achieve a balanced class distribution, which can improve the performance of our model.\nModifying the learning algorithms to increase their sensitivity to the minority class is also an effective strategy. This can involve adjusting the decision threshold in models like logistic regression or utilizing learning algorithms specifically designed to handle imbalanced datasets, such as Gradient Boosted Classifiers. These modifications can help the model pay more attention to the minority class and thus improve its ability to make accurate predictions.\nFinally, when dealing with imbalanced datasets, it’s crucial to use appropriate evaluation metrics. Accuracy might not provide a fair assessment of the model’s performance in such cases. Instead, metrics such as precision, recall, F1-score, or the Area Under the Receiver Operating Characteristic Curve (AUROC) can provide a more comprehensive understanding of the model’s performance, considering both the majority and minority classes. By focusing on these metrics, we can better evaluate and improve our model’s performance on imbalanced datasets.\nThe final phase of our data preparation was feature engineering. We identified ways to create new variables from the existing data that could better inform our model. These included user ratings and demographics, temporal trends, and movie metadata such as title, genre, release year, director/actor popularity, and the budget-to-revenue ratio. In the feature engineering step, we manipulated our data to create new, potentially more informative features, aiming to enhance the performance of our models. We focused primarily on profit-related features, as they offer valuable insights into a movie’s financial success.\nOne key feature genre is stored in json form which is not available for analysis on the first hand. We used techniques to deal with json type of data and converted those genres into one-hot-encoding due to its categorical nature.\nWe found that NA value in variable belongs-to-collection means that the movie does not belong to any collections. We believed it is a informative feature, so we turned it into a binary variable if-in-collection that takes value 1 and 0. Value 1 means that the movies belongs to a collection, and 0 means that the movie does not belong to any collection. In the future, we could collect the popularity of the collections which is known prior to the release of the new movie, and then add the interaction of the collection popularity with if-in-collection as a new feature in our model.\nWe constructed the Return on Investment (ROI) feature by subtracting the movie’s budget from the revenue and then dividing the result by the budget. This feature provides a measure of the profitability of the movie relative to its initial investment. It essentially tells us how much return we get for each dollar spent on the movie’s production.\nNext, we created the ‘box_office_failure_roi’ feature, a binary variable that uses the ROI to categorize movies as successful or unsuccessful. If a movie has a negative ROI, indicating that it made less money than was spent to produce it, we assigned a value of 1, signifying a box office failure. Conversely, if a movie has a non-negative ROI, suggesting that it broke even or made a profit, we assigned it a value of 0, marking it as a box office success.\nAnother feature we engineered was ‘Profitability’, a binary indicator that directly compares a movie’s revenue to its budget. If a movie’s revenue exceeds its budget, indicating a profit, we assigned a flat value of 1; if not, we assigned a value of 0. This feature gives a straightforward measure of whether a movie made a profit or not.\nWe also created the ‘Popularity to Budget’ feature by dividing the popularity of a movie by its budget. This feature gives an idea of how popular a movie is relative to the amount spent on it. A high value for this feature would suggest that the movie was able to achieve significant popularity despite a relatively low budget, whereas a low value might indicate that even a high budget was not able to guarantee popularity. We also one-hot encoded the categorical features, such as language, and only kept entries of entries with more than 30 observations to remove outliers. Through these steps, we aimed to better capture the underlying patterns and relationships in our data, providing our models with the necessary information to make accurate predictions.\nIn order to utilize more features we have in hand, we wanted to convert some text relating to the success of the movie into usable feature. Thanks to the innovation in Large Language Model (LLM), we had it as a powerful tool to do sentiment analysis on the overview of each movie. We used the OpenAI API to access the GPT 3.5 model, did prompt engineering which we tested several prompts and finally chose one that provided the best output, and then cleaned the output to analyzable format. We combine the overview of each movie with our prompt that asked the GPT 3.5 model to rate whether the movie overview is interesting in a scale of 0 to 10 (normalized to 0 to 1 later), and then use the GPT 3.5 model to generate the response. We used it as an additional feature in one of our three sets of variables later.\nAnalysis In the analysis phase of our study, we adopted a range of predictive modeling techniques, each aiming to forecast whether a movie would end up as a box office failure. We utilized logistic regression, random forests, and gradient boosted classifiers to experiment and test a wide range of machine learning models that achieve good performance in similar classification tasks.\nLogistic regression offers simplicity and interpretability, outputting results that can be understood in terms of probabilities. Random forests, on the other hand, is an ensemble method offering greater robustness and accuracy through decision tree aggregation, handling high-dimensionality and potential overfitting effectively. We also used XGBoost (Extreme Gradient Boosting), which is a sophisticated machine learning algorithm known for its speed, accuracy, and efficiency. It’s widely used for various predictive tasks including classification and regression and provides parallel processing, effective handling of missing values, and a regularization parameter to prevent overfitting, all of which contribute to its strong performance in generating highly accurate models.\nIn hopes of our models generalizing well to new data, we took steps to ensure they were not merely overfitting to our current dataset. We implemented feature scaling and normalization to treat all our variables on an even playing field and reduce the chance of higher-magnitude features unduly influencing the model. Cross-validation, a crucial step in our process, was used to gauge how well our model would perform on unseen data. By dividing our dataset into multiple subsets and training our model on various combinations of these subsets, we could gain insight into our model’s performance variance and its capability to generalize. Regularization was another technique we utilized to prevent overfitting. By adding a penalty term to the loss function, we could keep the weights of our model’s features small, thus reducing model complexity and improving generalizability.\nLastly, evaluating the runtime of each model was crucial in our selection process. While accuracy is vital, a model that takes too long to run can be impractical, so we considered the trade-off between model complexity and runtime.\nResults Here we are predicting box office failure ROI using only meta data. The Random Forest model had the highest training accuracy (0.77) and test accuracy (0.70). The XGBoost model, however, had slightly better F1 and F1macro scores, indicating a better balance between precision and recall.\nThis table introduces GPT ratings as an additional feature in predicting box office failure ROI. Interestingly, all the models show a decreased test accuracy and F1 score compared to their performance in Table 1. This could imply that the GPT rating isn’t providing a significant benefit to the model’s predictive power. The Random Forest model still performs the best in terms of training accuracy, but the XGBoost model surpasses it slightly on the F1 and F1 macro average scores.\nIn this table, post-release data is added to the meta data for predicting box office failure ROI. Here, we see a considerable improvement in all models’ performance metrics. The Random Forest and XGBoost models perform particularly well, with perfect training accuracy and very high test accuracy and F1 scores. The Cross-Validation (CV) score is also quite high for all models, showing their ability to generalize well.\nThis table focuses on predicting profitability using meta data and post-release data. Here, we see a significant drop in the F1 score compared to Table 3, despite maintaining high training and testing accuracies. This could imply that the models are not good at balancing precision and recall in this task. The Random Forest and XGBoost models are again the best performers, but the low F1 scores indicate a need for model improvement or feature selection refinement.\nThe Random Forest and XGBoost models generally outperformed the Logistic Regression model. However, their performances varied depending on the task and the features used. It also seems that post-release data significantly improves the models’ predictive capabilities. The GPT ratings, on the other hand, did not seem to provide significant benefit. These insights could help refine future models and feature selections for better predictive performance.\nIn the above analysis, the models using post-release data provided much higher performance than models using meta data only or meta data with GPT ratings. While this highlights the potential impact and value of post-release data in improving prediction accuracy, it also raises a critical practical challenge as this kind of data is typically not available until after a movie has been released, which is often too late to inform many important decisions about marketing, distribution, and other strategies aimed at maximizing box office success. Therefore, while including post-release data significantly improved the models’ predictive performance, such models are not practically useful in most real-world scenarios where predictions are needed before the movie release.\nGiven the limitations of using post-release data, focusing on improving predictive performance with only pre-release data (like meta data and GPT ratings) becomes essential. The comparison between Tables 1 and 2 shows that the addition of GPT ratings did not enhance the models’ performance. This suggests that, at least in this specific case, the GPT ratings did not add much value to the predictive power derived from the meta data. More in-depth investigation is needed to understand why GPT ratings did not improve performance - it could be due to the quality or relevance of the ratings, or the way they are being used by the models. Further exploration of other pre-release data, or more nuanced ways of utilizing the GPT ratings, might enhance the models’ predictive power in practical scenarios where only pre-release data is available.\nThe Random Forest and XGBoost models consistently outperformed the Logistic Regression model across all scenarios, which suggests that more complex, non-linear models may be better suited to capturing the nuances of box office success. However, it’s important to note that all models struggled with the profitability prediction task, as seen in Table 4, indicating that this is a challenging problem that may require more sophisticated modeling approaches or a different selection of features. While these models show promise, the practical applicability of predictive models for box office success depends heavily on the timing and availability of data, as well as on the complexity of the problem being addressed. Further work is needed to improve performance using only pre-release data, which is what would be available in real-world forecasting scenarios.\nConclusion In conclusion, our study as film-interested students provided an intriguing exploration into the utility of machine learning models for predicting box office performance and profitability. The most important findings from our study suggest that the use of post-release data significantly improves the prediction of box office failures based on return on investment; however, this kind of data is typically not available until after a movie has been released, which is often too late to inform many important decisions about marketing, distribution, and other strategies aimed at maximizing box office success.. Models such as Random Forest and XGBoost demonstrated promising results with high training, testing accuracy, and F1 scores.\nAdditionally, our research underscored the importance of carefully managing and processing data in the context of a real-world application. We confronted and navigated multiple challenges, including dealing with missing values, joining multiple large datasets, and understanding the context of the data, all of which contributed to our practical learning experience. This hands-on experience not only deepened our understanding of the complexities of data science but also provided us valuable insights into the film industry.\nDespite the challenges encountered, our collective interest in film and cinema motivated us, making the project both fascinating and rewarding. This project provides insights into the potential of machine learning in the film industry, and we hope our findings and experiences can serve as a stepping stone for other students and researchers venturing into this exciting intersection of data science and film.\n","wordCount":"3701","inLanguage":"en","image":"https://horizonhou.github.io/projects/machine-learning-llm/covers/BoxOffice.jpg","datePublished":"2023-05-26T23:15:00+07:00","dateModified":"2023-05-26T23:15:00+07:00","author":[{"@type":"Person","name":"Ruize Hou"},{"@type":"Person","name":"Hongjiao Zhang"},{"@type":"Person","name":"Jiao Wang"},{"@type":"Person","name":"Zijin Gao"},{"@type":"Person","name":"Curtis Chen"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://horizonhou.github.io/projects/machine-learning-llm/"},"publisher":{"@type":"Organization","name":"Ruize Hou","logo":{"@type":"ImageObject","url":"https://horizonhou.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://horizonhou.github.io/ accesskey=h title="Ruize Hou (Alt + H)"><img src=https://horizonhou.github.io/images/data-science.png alt=logo aria-label=logo height=35>Ruize Hou</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://horizonhou.github.io/about/ title=About><span>About</span></a></li><li><a href=https://horizonhou.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://horizonhou.github.io/blogs/ title=Blogs><span>Blogs</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://horizonhou.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://horizonhou.github.io/projects/>Projects</a></div><h1 class=post-title>Predicting Box Office Failures with LLM</h1><div class=post-meta><span title='2023-05-26 23:15:00 +0700 +0700'>May 26, 2023</span>&nbsp;·&nbsp;18 min&nbsp;·&nbsp;Ruize Hou, Hongjiao Zhang, Jiao Wang, Zijin Gao, Curtis Chen</div></header><figure class=entry-cover><img loading=lazy src=https://horizonhou.github.io/covers/BoxOffice.jpg alt></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#abstract aria-label=Abstract>Abstract</a></li><li><a href=#sec%3aintro aria-label=Introduction>Introduction</a></li><li><a href=#sec%3aformatting aria-label="Related Work">Related Work</a></li><li><a href=#sec%3aformatting aria-label=Dataset>Dataset</a></li><li><a href=#data-preprocessing aria-label="Data Preprocessing">Data Preprocessing</a></li><li><a href=#analysis aria-label=Analysis>Analysis</a></li><li><a href=#results aria-label=Results>Results</a></li><li><a href=#sec%3aformatting aria-label=Conclusion>Conclusion</a></li></ul></div></details></div><div class=post-content><hr><h1 id=abstract>Abstract<a hidden class=anchor aria-hidden=true href=#abstract>#</a></h1><p>This report presents a comprehensive data science study designed to
predict box office performance of movies, leveraging an integrated
analysis of three extensive datasets: MovieLens 25M, IMDb
Non-commericial, and Kaggle Movie Revenue. These datasets collectively
provide a wealth of movie features, including ratings, revenue,
metadata, and audience preferences. The study primarily aims to
identify the key variables that influence a movie&rsquo;s box office success
or failure, seeking to unravel the potential impact of a movie&rsquo;s
budget on its box office performance, the likelihood of certain genres
failing at the box office, and the strategies that can be optimized to
maximize returns on movie production investments. We also delve into
the role of marketing expenses in shaping a movie&rsquo;s box office
performance, providing actionable insights that can help refine and
enhance the effectiveness of marketing campaigns. The datasets used in
this project are extensive, with the MovieLens dataset encompassing 25
million ratings applied to 62,000 movies by 162,000 users, the IMDb
dataset providing a vast trove of movie data, and the Kaggle Revenue
dataset offering 26 million ratings from 270,000 users for 45,000
movies. Rigorous data cleaning techniques were applied to ensure the
integrity and accuracy of the datasets. For our prediction models, we
utilized a range of machine learning algorithms, including Random
Forest, Logistic Regression, XGBoost, and Gradient Boosted Trees,
achieving a testing accuracy of around 0.92. The implications of this
research are substantial for the film industry. By offering predictive
models and insights that can help mitigate the risk of box office
failure, optimize both production and marketing strategies, and
enhance the return on investment for movie projects, this study
contributes significantly to understanding audience preferences and
establishing a pathway towards increased profitability in the movie
industry.</p><h1 id=sec:intro>Introduction<a hidden class=anchor aria-hidden=true href=#sec:intro>#</a></h1><p>In an industry where the thin line between success and failure often
lies in the delicate balance of creativity, budgeting, and marketing,
understanding the dynamics of box office performance is crucial. As a
group of students with a shared passion for film and movies, we were
driven to use our data science skills to delve into the intriguing
complexities of the film industry. The motivation behind our study was
not just academic curiosity; we sought to contribute tangible insights
that could inform and influence the industry&rsquo;s approach to production
and investment strategies.</p><p>Our project aims to predict box office success by analyzing an extensive
array of movie features and audience preferences. To achieve this, we
have combined three expansive datasets: MovieLens, IMDb, and Kaggle
Revenue. These datasets collectively provide a wealth of data, including
ratings, revenue, and metadata for thousands of movies. Specifically,
the MovieLens dataset comprises 25 million ratings applied to 62,000
movies by 162,000 users, the IMDb dataset offers a comprehensive array
of movie data, and the Kaggle Revenue dataset presents 26 million
ratings from 270,000 users for 45,000 movies.</p><p>The scope of our study involves identifying the key variables that
influence a movie&rsquo;s box office performance. We intend to unravel
questions like the impact of a movie&rsquo;s budget on its box office
performance, the susceptibility of certain genres to failure, and the
role of marketing expenses in shaping a movie&rsquo;s financial success. Our
goal is not only to provide predictive models for box office success but
also to generate insights that can help optimize production and
marketing strategies.</p><p>By merging our passion for the film industry with our expertise in data
science, we aim to shed light on the factors that contribute to a
movie&rsquo;s success or failure at the box office. We believe that our
research will serve as a valuable resource for film studios and
investors, providing them with data-driven insights to optimize their
decisions and investments in the movie industry.</p><h1 id=sec:formatting>Related Work<a hidden class=anchor aria-hidden=true href=#sec:formatting>#</a></h1><p>In this section, we summarize some work that is most relevant to ours.</p><p>Our study builds upon a rich body of academic research that has applied
data science to the film industry. A seminal resource for our work was
the MovieLens dataset, the creation and application of which was
discussed in Harper and Konstan&rsquo;s "The MovieLens Datasets: History and
Context" [@harper2015movielens]. This dataset, encompassing 25 million
ratings applied to 62,000 movies, has proven invaluable in numerous
studies for understanding audience preferences and movie ratings.</p><p>Simultaneously, sentiment analysis has emerged as a significant tool in
this field. Asano and Motomura&rsquo;s "Large-Scale Movie Review Dataset for
Sentiment Analysis" [@asano2015large] demonstrates the utility of
sentiment analysis in understanding public opinion about movies, an
approach that complements traditional numerical ratings.</p><p>The question of whether critical reviews influence box office success
has also been examined. Maltby&rsquo;s study "Predicting box-office success:
do critical reviews really matter?" [@maltby2000predicting] provided
insights into the interplay between critical opinions and audience
reception, an important factor in determining a movie&rsquo;s financial
performance.</p><p>In recent years, machine learning has been increasingly leveraged to
predict box office performance. Narayanan et al.&rsquo;s study
[@narayanan2019box] and Subramaniyaswamy et al.&rsquo;s research
[@2017subramaniyaswamy] both employed machine learning algorithms to
predict box office success, laying the groundwork for our own predictive
models. These models have been further refined by studies such as "A
Machine Learning Approach to Predict Movie Revenue Based on Pre-Released
Movie Metadata" [@ML2020] and "Finding Nemo: Predicting Movie
Performances by Machine Learning Methods" [@Nemo2020], which emphasize
the predictive power of pre-released movie metadata.</p><p>Kim et al.&rsquo;s study [@SNS2015] and Shen&rsquo;s research [@Shen2020] took a
different approach by integrating social media data into their machine
learning models. They demonstrated that the sentiment expressed on
social media platforms could be a valuable predictor of box office
performance, a finding that underscores the importance of considering a
wide range of data sources in our study.</p><p>Our research aligns with and extends upon a significant body of existing
academic work. We aspire to contribute to this ongoing dialogue by
providing new insights into the predictors of box office success and
exploring the potential for data science to optimize film industry
practices.</p><h1 id=sec:formatting>Dataset<a hidden class=anchor aria-hidden=true href=#sec:formatting>#</a></h1><p>Our study&rsquo;s dataset is a robust combination of three widely-recognized
sources, namely the MovieLens dataset, the IMDb dataset, and the Revenue
dataset from Kaggle. These datasets collectively present a comprehensive
array of movie features essential for our research.</p><p>The MovieLens dataset, developed by the GroupLens research lab at the
University of Minnesota, is an extensive resource that contains 25
million ratings and one million tag applications. These ratings and tags
have been applied to a total of 62,000 movies by an active user base of
162,000 individuals. This dataset allows us to access a vast and diverse
range of user opinions on a large collection of movies.</p><p>IMDb is a renowned online database that provides a plethora of
information about films, television programs, and video games, including
cast, production crew, plot summaries, and ratings. This dataset
provides us with a broad range of movie metadata, offering deeper
insights into the features that might influence a movie&rsquo;s box office
performance.</p><p>Lastly, the Revenue dataset from Kaggle includes 26 million ratings from
270,000 users for 45,000 movies, is a rich source of revenue data. It
allows us to study the relationship between various movie features and
their box office performance.</p><p>The combined dataset presents a comprehensive collection of movie
features, ratings, revenue data, and other associated metadata. The
breadth and depth of these datasets provide a strong foundation for our
analysis and predictive modeling in this study.</p><h1 id=data-preprocessing>Data Preprocessing<a hidden class=anchor aria-hidden=true href=#data-preprocessing>#</a></h1><p>Our data-driven exploration began with a comprehensive process of
preparing and understanding our data. Exploratory Data Analysis (EDA)
was a critical part of this process. We examined our data using various
statistical and visualization tools to understand the features and
relationships within our dataset. Our exploration included the creation
of histograms to understand the distribution of our data, boxplots to
identify outliers, and a coefficient matrix to examine the relationships
between variables.</p><p>Through outlier analysis, we found that there exists many outliers in
budget and revenue. There are many successful movies that have enormous
investment in production and outstanding performance. However, we didn&rsquo;t
want to remove these outliers because we want to take all types of
movie, no matter it is big production or not, into consideration. We
will mitigate the outlier effect by making our target variable binary
and also through mitigating the imbalance of the data.</p><p>The insights from our EDA guided our data cleaning process. We
identified and addressed anomalies such as duplicate entries and missing
data. We also did our best to fill in gaps in our dataset where the data
was numerical. For example, there were missing &lsquo;budget&rsquo; values, which we
estimated using other features like &lsquo;year&rsquo; and &lsquo;genre&rsquo;, or using online
resources to impute missing values. Figure [4] is the
data without preprocessing and figure [3] is the data after removing entries with
missing budgets, which are entries with budget of 0 in the data.</p><p><img loading=lazy src=/images/MoviePrediction/eda1.png alt=image1></p><p>We also acknowledged the potential for bias in our dataset. The IMDb
dataset may favor specific movie types or genres, and this had to be
addressed to ensure fair analysis. We mitigated this by integrating box
office revenue data into our analysis to complement the IMDb data.
Resolving class imbalance can begin with resampling techniques. This
process involves adjusting the dataset by either oversampling the
minority class, which duplicates instances from the minority class, or
undersampling the majority class by removing instances. The goal is to
achieve a balanced class distribution, which can improve the performance
of our model.</p><p>Modifying the learning algorithms to increase their sensitivity to the
minority class is also an effective strategy. This can involve adjusting
the decision threshold in models like logistic regression or utilizing
learning algorithms specifically designed to handle imbalanced datasets,
such as Gradient Boosted Classifiers. These modifications can help the
model pay more attention to the minority class and thus improve its
ability to make accurate predictions.</p><p>Finally, when dealing with imbalanced datasets, it&rsquo;s crucial to use
appropriate evaluation metrics. Accuracy might not provide a fair
assessment of the model&rsquo;s performance in such cases. Instead, metrics
such as precision, recall, F1-score, or the Area Under the Receiver
Operating Characteristic Curve (AUROC) can provide a more comprehensive
understanding of the model&rsquo;s performance, considering both the majority
and minority classes. By focusing on these metrics, we can better
evaluate and improve our model&rsquo;s performance on imbalanced datasets.</p><p><img loading=lazy src=/images/MoviePrediction/figure5.png alt=image1>
<img loading=lazy src=/images/MoviePrediction/figure6.png alt=image1></p><p>The final phase of our data preparation was feature engineering. We
identified ways to create new variables from the existing data that
could better inform our model. These included user ratings and
demographics, temporal trends, and movie metadata such as title, genre,
release year, director/actor popularity, and the budget-to-revenue
ratio. In the feature engineering step, we manipulated our data to
create new, potentially more informative features, aiming to enhance the
performance of our models. We focused primarily on profit-related
features, as they offer valuable insights into a movie&rsquo;s financial
success.</p><p>One key feature genre is stored in json form which is not available for
analysis on the first hand. We used techniques to deal with json type of
data and converted those genres into one-hot-encoding due to its
categorical nature.</p><p>We found that NA value in variable belongs-to-collection means that the
movie does not belong to any collections. We believed it is a
informative feature, so we turned it into a binary variable
if-in-collection that takes value 1 and 0. Value 1 means that the movies
belongs to a collection, and 0 means that the movie does not belong to
any collection. In the future, we could collect the popularity of the
collections which is known prior to the release of the new movie, and
then add the interaction of the collection popularity with
if-in-collection as a new feature in our model.</p><p>We constructed the Return on Investment (ROI) feature by subtracting the
movie&rsquo;s budget from the revenue and then dividing the result by the
budget. This feature provides a measure of the profitability of the
movie relative to its initial investment. It essentially tells us how
much return we get for each dollar spent on the movie&rsquo;s production.</p><p>Next, we created the &lsquo;box_office_failure_roi&rsquo; feature, a binary variable
that uses the ROI to categorize movies as successful or unsuccessful. If
a movie has a negative ROI, indicating that it made less money than was
spent to produce it, we assigned a value of 1, signifying a box office
failure. Conversely, if a movie has a non-negative ROI, suggesting that
it broke even or made a profit, we assigned it a value of 0, marking it
as a box office success.</p><p>Another feature we engineered was &lsquo;Profitability&rsquo;, a binary indicator
that directly compares a movie&rsquo;s revenue to its budget. If a movie&rsquo;s
revenue exceeds its budget, indicating a profit, we assigned a flat
value of 1; if not, we assigned a value of 0. This feature gives a
straightforward measure of whether a movie made a profit or not.</p><p>We also created the &lsquo;Popularity to Budget&rsquo; feature by dividing the
popularity of a movie by its budget. This feature gives an idea of how
popular a movie is relative to the amount spent on it. A high value for
this feature would suggest that the movie was able to achieve
significant popularity despite a relatively low budget, whereas a low
value might indicate that even a high budget was not able to guarantee
popularity. We also one-hot encoded the categorical features, such as
language, and only kept entries of entries with more than 30
observations to remove outliers. Through these steps, we aimed to better
capture the underlying patterns and relationships in our data, providing
our models with the necessary information to make accurate predictions.</p><p><img loading=lazy src=/images/MoviePrediction/llm.png alt=image1></p><p>In order to utilize more features we have in hand, we wanted to convert
some text relating to the success of the movie into usable feature.
Thanks to the innovation in Large Language Model (LLM), we had it as a
powerful tool to do sentiment analysis on the overview of each movie. We
used the OpenAI API to access the GPT 3.5 model, did prompt engineering
which we tested several prompts and finally chose one that provided the
best output, and then cleaned the output to analyzable format. We
combine the overview of each movie with our prompt that asked the GPT
3.5 model to rate whether the movie overview is interesting in a scale
of 0 to 10 (normalized to 0 to 1 later), and then use the GPT 3.5 model
to generate the response. We used it as an additional feature in one of
our three sets of variables later.</p><h1 id=analysis>Analysis<a hidden class=anchor aria-hidden=true href=#analysis>#</a></h1><p>In the analysis phase of our study, we adopted a range of predictive
modeling techniques, each aiming to forecast whether a movie would end
up as a box office failure. We utilized logistic regression, random
forests, and gradient boosted classifiers to experiment and test a wide
range of machine learning models that achieve good performance in
similar classification tasks.</p><p>Logistic regression offers simplicity and interpretability, outputting
results that can be understood in terms of probabilities. Random
forests, on the other hand, is an ensemble method offering greater
robustness and accuracy through decision tree aggregation, handling
high-dimensionality and potential overfitting effectively. We also used
XGBoost (Extreme Gradient Boosting), which is a sophisticated machine
learning algorithm known for its speed, accuracy, and efficiency. It&rsquo;s
widely used for various predictive tasks including classification and
regression and provides parallel processing, effective handling of
missing values, and a regularization parameter to prevent overfitting,
all of which contribute to its strong performance in generating highly
accurate models.</p><p>In hopes of our models generalizing well to new data, we took steps to
ensure they were not merely overfitting to our current dataset. We
implemented feature scaling and normalization to treat all our variables
on an even playing field and reduce the chance of higher-magnitude
features unduly influencing the model. Cross-validation, a crucial step
in our process, was used to gauge how well our model would perform on
unseen data. By dividing our dataset into multiple subsets and training
our model on various combinations of these subsets, we could gain
insight into our model&rsquo;s performance variance and its capability to
generalize. Regularization was another technique we utilized to prevent
overfitting. By adding a penalty term to the loss function, we could
keep the weights of our model&rsquo;s features small, thus reducing model
complexity and improving generalizability.</p><p>Lastly, evaluating the runtime of each model was crucial in our
selection process. While accuracy is vital, a model that takes too long
to run can be impractical, so we considered the trade-off between model
complexity and runtime.</p><h1 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h1><p>Here we are predicting box office failure ROI using only meta data. The
Random Forest model had the highest training accuracy (0.77) and test
accuracy (0.70). The XGBoost model, however, had slightly better F1 and
F1macro scores, indicating a better balance between precision and
recall.</p><p><img loading=lazy src=/images/MoviePrediction/table1.png alt=image1></p><p>This table introduces GPT ratings as an additional feature in predicting
box office failure ROI. Interestingly, all the models show a decreased
test accuracy and F1 score compared to their performance in Table 1.
This could imply that the GPT rating isn&rsquo;t providing a significant
benefit to the model&rsquo;s predictive power. The Random Forest model still
performs the best in terms of training accuracy, but the XGBoost model
surpasses it slightly on the F1 and F1 macro average scores.</p><p><img loading=lazy src=/images/MoviePrediction/table2.png alt=image1></p><p>In this table, post-release data is added to the meta data for
predicting box office failure ROI. Here, we see a considerable
improvement in all models&rsquo; performance metrics. The Random Forest and
XGBoost models perform particularly well, with perfect training accuracy
and very high test accuracy and F1 scores. The Cross-Validation (CV)
score is also quite high for all models, showing their ability to
generalize well.</p><p><img loading=lazy src=/images/MoviePrediction/table3.png alt=image1></p><p>This table focuses on predicting profitability using meta data and
post-release data. Here, we see a significant drop in the F1 score
compared to Table 3, despite maintaining high training and testing
accuracies. This could imply that the models are not good at balancing
precision and recall in this task. The Random Forest and XGBoost models
are again the best performers, but the low F1 scores indicate a need for
model improvement or feature selection refinement.</p><p><img loading=lazy src=/images/MoviePrediction/table4.png alt=image1></p><p>The Random Forest and XGBoost models generally outperformed the Logistic
Regression model. However, their performances varied depending on the
task and the features used. It also seems that post-release data
significantly improves the models&rsquo; predictive capabilities. The GPT
ratings, on the other hand, did not seem to provide significant benefit.
These insights could help refine future models and feature selections
for better predictive performance.</p><p>In the above analysis, the models using post-release data provided much
higher performance than models using meta data only or meta data with
GPT ratings. While this highlights the potential impact and value of
post-release data in improving prediction accuracy, it also raises a
critical practical challenge as this kind of data is typically not
available until after a movie has been released, which is often too late
to inform many important decisions about marketing, distribution, and
other strategies aimed at maximizing box office success. Therefore,
while including post-release data significantly improved the models&rsquo;
predictive performance, such models are not practically useful in most
real-world scenarios where predictions are needed before the movie
release.</p><p>Given the limitations of using post-release data, focusing on improving
predictive performance with only pre-release data (like meta data and
GPT ratings) becomes essential. The comparison between Tables 1 and 2
shows that the addition of GPT ratings did not enhance the models&rsquo;
performance. This suggests that, at least in this specific case, the GPT
ratings did not add much value to the predictive power derived from the
meta data. More in-depth investigation is needed to understand why GPT
ratings did not improve performance - it could be due to the quality or
relevance of the ratings, or the way they are being used by the models.
Further exploration of other pre-release data, or more nuanced ways of
utilizing the GPT ratings, might enhance the models&rsquo; predictive power in
practical scenarios where only pre-release data is available.</p><p>The Random Forest and XGBoost models consistently outperformed the
Logistic Regression model across all scenarios, which suggests that more
complex, non-linear models may be better suited to capturing the nuances
of box office success. However, it&rsquo;s important to note that all models
struggled with the profitability prediction task, as seen in Table 4,
indicating that this is a challenging problem that may require more
sophisticated modeling approaches or a different selection of features.
While these models show promise, the practical applicability of
predictive models for box office success depends heavily on the timing
and availability of data, as well as on the complexity of the problem
being addressed. Further work is needed to improve performance using
only pre-release data, which is what would be available in real-world
forecasting scenarios.</p><h1 id=sec:formatting>Conclusion<a hidden class=anchor aria-hidden=true href=#sec:formatting>#</a></h1><p>In conclusion, our study as film-interested students provided an
intriguing exploration into the utility of machine learning models for
predicting box office performance and profitability. The most important
findings from our study suggest that the use of post-release data
significantly improves the prediction of box office failures based on
return on investment; however, this kind of data is typically not
available until after a movie has been released, which is often too late
to inform many important decisions about marketing, distribution, and
other strategies aimed at maximizing box office success.. Models such as
Random Forest and XGBoost demonstrated promising results with high
training, testing accuracy, and F1 scores.</p><p>Additionally, our research underscored the importance of carefully
managing and processing data in the context of a real-world application.
We confronted and navigated multiple challenges, including dealing with
missing values, joining multiple large datasets, and understanding the
context of the data, all of which contributed to our practical learning
experience. This hands-on experience not only deepened our understanding
of the complexities of data science but also provided us valuable
insights into the film industry.</p><p>Despite the challenges encountered, our collective interest in film and
cinema motivated us, making the project both fascinating and rewarding.
This project provides insights into the potential of machine learning in
the film industry, and we hope our findings and experiences can serve as
a stepping stone for other students and researchers venturing into this
exciting intersection of data science and film.</p></div><footer class=post-footer><nav class=paginav><a class=next href=https://horizonhou.github.io/projects/logistics-optimization/><span class=title>Next Page »</span><br><span>Express Air Cargo Optimization</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Predicting Box Office Failures with LLM on twitter" href="https://twitter.com/intent/tweet/?text=Predicting%20Box%20Office%20Failures%20with%20LLM&amp;url=https%3a%2f%2fhorizonhou.github.io%2fprojects%2fmachine-learning-llm%2f&amp;hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Predicting Box Office Failures with LLM on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fhorizonhou.github.io%2fprojects%2fmachine-learning-llm%2f&amp;title=Predicting%20Box%20Office%20Failures%20with%20LLM&amp;summary=Predicting%20Box%20Office%20Failures%20with%20LLM&amp;source=https%3a%2f%2fhorizonhou.github.io%2fprojects%2fmachine-learning-llm%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Predicting Box Office Failures with LLM on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fhorizonhou.github.io%2fprojects%2fmachine-learning-llm%2f&title=Predicting%20Box%20Office%20Failures%20with%20LLM"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Predicting Box Office Failures with LLM on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fhorizonhou.github.io%2fprojects%2fmachine-learning-llm%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Predicting Box Office Failures with LLM on whatsapp" href="https://api.whatsapp.com/send?text=Predicting%20Box%20Office%20Failures%20with%20LLM%20-%20https%3a%2f%2fhorizonhou.github.io%2fprojects%2fmachine-learning-llm%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Predicting Box Office Failures with LLM on telegram" href="https://telegram.me/share/url?text=Predicting%20Box%20Office%20Failures%20with%20LLM&amp;url=https%3a%2f%2fhorizonhou.github.io%2fprojects%2fmachine-learning-llm%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://horizonhou.github.io/>Ruize Hou</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>