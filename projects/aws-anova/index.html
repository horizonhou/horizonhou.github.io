<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Researching the Impact of Multiple Factors in Speech Transcription Using Amazon Transcribe Service | Ruize Hou</title><meta name=keywords content><meta name=description content="Topic: Researching the Impact of Multiple Factors in Speech Transcription Using Amazon Transcribe Service Introduction Audio is a popular source of content expression, ranging from music, podcast, to zoom classes. Audio not only enhances multimedia applications by serving as an extra channel of information but also adds a sense of realism by conveying emotion, time period, and geographical location of the speaker. During the Covid-19 pandemic in which social distancing is required, audio has become an increasingly important channel for people to conduct their businesses and connect with each other."><meta name=author content="Ruize Hou, Angel Song, Sirui Hu, Aoran Wang, Julin Ye"><link rel=canonical href=https://horizonhou.github.io/projects/aws-anova/><link crossorigin=anonymous href=/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://horizonhou.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://horizonhou.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://horizonhou.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://horizonhou.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://horizonhou.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Researching the Impact of Multiple Factors in Speech Transcription Using Amazon Transcribe Service"><meta property="og:description" content="Topic: Researching the Impact of Multiple Factors in Speech Transcription Using Amazon Transcribe Service Introduction Audio is a popular source of content expression, ranging from music, podcast, to zoom classes. Audio not only enhances multimedia applications by serving as an extra channel of information but also adds a sense of realism by conveying emotion, time period, and geographical location of the speaker. During the Covid-19 pandemic in which social distancing is required, audio has become an increasingly important channel for people to conduct their businesses and connect with each other."><meta property="og:type" content="article"><meta property="og:url" content="https://horizonhou.github.io/projects/aws-anova/"><meta property="og:image" content="https://horizonhou.github.io/projects/aws-anova/covers/AWSTranscribe.jpg"><meta property="article:section" content="projects"><meta property="article:published_time" content="2021-05-15T23:15:00+07:00"><meta property="article:modified_time" content="2021-05-15T23:15:00+07:00"><meta property="og:site_name" content="RuizeHou"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://horizonhou.github.io/projects/aws-anova/covers/AWSTranscribe.jpg"><meta name=twitter:title content="Researching the Impact of Multiple Factors in Speech Transcription Using Amazon Transcribe Service"><meta name=twitter:description content="Topic: Researching the Impact of Multiple Factors in Speech Transcription Using Amazon Transcribe Service Introduction Audio is a popular source of content expression, ranging from music, podcast, to zoom classes. Audio not only enhances multimedia applications by serving as an extra channel of information but also adds a sense of realism by conveying emotion, time period, and geographical location of the speaker. During the Covid-19 pandemic in which social distancing is required, audio has become an increasingly important channel for people to conduct their businesses and connect with each other."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"https://horizonhou.github.io/projects/"},{"@type":"ListItem","position":2,"name":"Researching the Impact of Multiple Factors in Speech Transcription Using Amazon Transcribe Service","item":"https://horizonhou.github.io/projects/aws-anova/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Researching the Impact of Multiple Factors in Speech Transcription Using Amazon Transcribe Service","name":"Researching the Impact of Multiple Factors in Speech Transcription Using Amazon Transcribe Service","description":"Topic: Researching the Impact of Multiple Factors in Speech Transcription Using Amazon Transcribe Service Introduction Audio is a popular source of content expression, ranging from music, podcast, to zoom classes. Audio not only enhances multimedia applications by serving as an extra channel of information but also adds a sense of realism by conveying emotion, time period, and geographical location of the speaker. During the Covid-19 pandemic in which social distancing is required, audio has become an increasingly important channel for people to conduct their businesses and connect with each other.","keywords":[],"articleBody":"Topic: Researching the Impact of Multiple Factors in Speech Transcription Using Amazon Transcribe Service Introduction Audio is a popular source of content expression, ranging from music, podcast, to zoom classes. Audio not only enhances multimedia applications by serving as an extra channel of information but also adds a sense of realism by conveying emotion, time period, and geographical location of the speaker. During the Covid-19 pandemic in which social distancing is required, audio has become an increasingly important channel for people to conduct their businesses and connect with each other. For example, many companies adopted audio calls as a method to replace traditional meetings and schools delivered online lectures to avoid gatherings. Audio transcription, as a complementary service for audio, converts speech in an audio file into written text. It has a wide range of applications, such as extracting information from interviews, analyzing patterns in academic research and even adding subtitles to lecture recording. Thus, the accuracy of audio transcription becomes critical to whether or not the users can correctly and efficiently understand the audio and apply the results to their work field. Traditionally, users had to rely on transcription providers to manually listen and transcribe the audio into text, which was costly, inefficient and in poor accuracy. Thanks to the rapid development of artificial intelligence and machine learning technology, now we can use a lot of online transcription services to help us achieve our goals.\nIn this report, we are going to examine two factors speech speed, and accent, that could potentially affect the accuracy of audio transcription by measuring the word error rate(WER). We adopt [method] to generate 55 different texts by altering speech speed, and accent, and transcribe them to texts using Amazon Transcribe Service. We then calculate the corresponding word error rate for each speed and accent and analyze the results using a wide range of python statistics tools. Our assumption before data analysis is that first, when the speech speed increases, the accuracy will decrease holding other factors fixed, second, the accuracy of US accent will be higher than others. Based on the results, both accents and speed influence on Amazon transcribe accuracy, while accent poses a stronger effect than speed.\nMethod Overview The core method in our project is Amazon Transcribe Services, which uses automatic speech recognition(ASR), a deep learning process, to convert speech to text quickly with high accuracy. The method enjoys four main benefits. First, it creates easily readable transcriptions by adding speaker diarization, punctuation and formatting. Second, it ensures customers privacy by identifying and redacting sensitive personally identifiable information and allowing contact centers to easily review and share the transcripts for the purpose of customer experience insight and agent training based on the PII. Third, it increases accuracy with customized transcriptions by allowing users to add new words to base vocabulary, such as product names, technical terminology, etc. or train the language model. Fourth, it filters specific words by enabling the user to mask or remove words that are unwanted.\nResearch Design We initially input SSML scripts to a Google text to audio converting service named Google TTS Audio Service, from which we generated two groups of audio files: ones with same speeds and different accents and ones with same accents and different speeds. To proceed, we uploaded them into two separate Amazon S3 buckets, and utilized Amazon Transcribe service to convert them to our source text files. Afterwards, we used the Jiwer speech evaluation recognition package to build the WER (world error rate) dataframe, where we compare the target original input SSML scripts and source text files we generated using Amazon Transcribe service. Finally, we input the WER dataframe to ANOVA analysis to testify whether our results are statistically significant.\nData Preparation We generated raw audio files using an online voice-generator which applies the Google TTS Audio Service. Although Amazon has its own text-to-audio service named Amazon Polly, it has only four different accents available, which does not provide us enough sample data for the research. On the contrary Google TTS provides a variety of The content includes five different texts as followed, ranging from novels and website contents to news articles:\nAs the research aims to analyze the effect of accents and speed on the accuracy of text generated by Amazon Transcribe. The raw audios cover six different accents including French, Spanish, Japanese, Korean, UK, and US. The audio files were also generated at five different speeds including 0.5,0.8,1.0,1.2, and 1.5 in the US accent.\nTranscribe Model Running After downloading audio files from s3 bucket, we run the transcribe model using our generated audio file. We divided our audio files into two parts. In the first part, we choose files based on six different accents with five different versions of text. In the second parts, we choose files based on five different accents with five different versions of texts. We use five different versions of texts in order to avoid the bias caused by one certain type of text. We use loops to run the transcribe and extract data from the Transcribe model. The results are saved into the text lists which contain all converted versions of text after running the Transcribe model.\nWord Error Rate Calculation In order to evaluate the accuracy of transcribed text generated by Amazon Transcribe, we introduce a metric called Word Error Rate (WER). This method is recommended by the US National Institute of Standards and Technology for the evaluation of ASR systems. WER is defined as the normalized Levenshtein edit distance. Levenshtein edit distance calculates the distance between the reference and the hypothesis.:\nThe formula is\nWER is the percentage of transcription errors produced by the ASR method compared to the number of words actually spoken. In other words, it’s the minimum number of words that need to be corrected to change the hypothesis transcript into the reference transcript, divided by the number of words that the speaker originally said. According to this formula, we know that the lower the WER, the more accurate the transcribed text is, with 0 error rate the best. WER can go beyond 1 if there are too many insertion errors.\nThe usage of WER is simple. This algorithm is contained in the jiwer package. First we should install it using the pip function. Then we import the wer function from the jiwer package After we prepared the reference text and hypothesis text we wanted to test on, we can call the wer() function to get the result (origin refers to reference text, test refer to hypothesis text), Calculating WER for all the transcribed text files we collected, we make our WER table and can move on to the statistical analysis part. Below is a glimpse of the WER dataframe. Data Visualization And Analysis According to our OLS Regression and data visualization, accents and speed do affect the accuracy of Amazon Transcribe. Amazon Transcribe results in accuracies for different accents which rank from the highest to lowest as US, UK, Spanish, French, Korean, and Japanese. Meanwhile, based on the regression, the positive coefficient of scripts generated from Japanese and Korean accents and WER suggest the error rate for these two accents tends to be higher compared to other accents, whereas the negative coefficient of scripts generated from UK, Spanish, and US accents and WER suggests a more accurate transcription.\nAs for the speed, the WER values of different speeds do not show a significant difference given a large p-value. From boxplot visualization, we can find that the average value of WER in the five different speeds falls in the range around 0.15. We also find that the difference between speed 0.8 which has the largest WER and speed 1.0 which has lowest WER are not significant, which corresponds to our main finding. Based on regression model analysis, the coefficients are all negative for the categorical speeds. If treating speed as a numerical variable, for every unit increase in speed, the WER value will decrease.\n","wordCount":"1326","inLanguage":"en","image":"https://horizonhou.github.io/projects/aws-anova/covers/AWSTranscribe.jpg","datePublished":"2021-05-15T23:15:00+07:00","dateModified":"2021-05-15T23:15:00+07:00","author":[{"@type":"Person","name":"Ruize Hou"},{"@type":"Person","name":"Angel Song"},{"@type":"Person","name":"Sirui Hu"},{"@type":"Person","name":"Aoran Wang"},{"@type":"Person","name":"Julin Ye"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://horizonhou.github.io/projects/aws-anova/"},"publisher":{"@type":"Organization","name":"Ruize Hou","logo":{"@type":"ImageObject","url":"https://horizonhou.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://horizonhou.github.io/ accesskey=h title="Ruize Hou (Alt + H)"><img src=https://horizonhou.github.io/images/data-science.png alt=logo aria-label=logo height=35>Ruize Hou</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://horizonhou.github.io/about/ title=About><span>About</span></a></li><li><a href=https://horizonhou.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://horizonhou.github.io/blogs/ title=Blogs><span>Blogs</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://horizonhou.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://horizonhou.github.io/projects/>Projects</a></div><h1 class=post-title>Researching the Impact of Multiple Factors in Speech Transcription Using Amazon Transcribe Service</h1><div class=post-meta><span title='2021-05-15 23:15:00 +0700 +0700'>May 15, 2021</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Ruize Hou, Angel Song, Sirui Hu, Aoran Wang, Julin Ye</div></header><figure class=entry-cover><img loading=lazy src=https://horizonhou.github.io/covers/AWSTranscribe.jpg alt></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#topic-researching-the-impact-of-multiple-factors-in-speech-transcription-using-amazon-transcribe-service aria-label="Topic: Researching the Impact of Multiple Factors in Speech Transcription Using Amazon Transcribe Service">Topic: Researching the Impact of Multiple Factors in Speech Transcription Using Amazon Transcribe Service</a><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#method-overview aria-label="Method Overview">Method Overview</a></li><li><a href=#research-design aria-label="Research Design">Research Design</a></li><li><a href=#data-preparation aria-label="Data Preparation">Data Preparation</a></li><li><a href=#transcribe-model-running aria-label="Transcribe Model Running">Transcribe Model Running</a></li><li><a href=#word-error-rate-calculation aria-label="Word Error Rate Calculation">Word Error Rate Calculation</a></li><li><a href=#data-visualization-and-analysis aria-label="Data Visualization And Analysis">Data Visualization And Analysis</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=topic-researching-the-impact-of-multiple-factors-in-speech-transcription-using-amazon-transcribe-service>Topic: Researching the Impact of Multiple Factors in Speech Transcription Using Amazon Transcribe Service<a hidden class=anchor aria-hidden=true href=#topic-researching-the-impact-of-multiple-factors-in-speech-transcription-using-amazon-transcribe-service>#</a></h1><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Audio is a popular source of content expression, ranging from music, podcast, to zoom classes. Audio not only enhances multimedia applications by serving as an extra channel of information but also adds a sense of realism by conveying emotion, time period, and geographical location of the speaker. During the Covid-19 pandemic in which social distancing is required, audio has become an increasingly important channel for people to conduct their businesses and connect with each other. For example, many companies adopted audio calls as a method to replace traditional meetings and schools delivered online lectures to avoid gatherings. Audio transcription, as a complementary service for audio, converts speech in an audio file into written text. It has a wide range of applications, such as extracting information from interviews, analyzing patterns in academic research and even adding subtitles to lecture recording. Thus, the accuracy of audio transcription becomes critical to whether or not the users can correctly and efficiently understand the audio and apply the results to their work field. Traditionally, users had to rely on transcription providers to manually listen and transcribe the audio into text, which was costly, inefficient and in poor accuracy. Thanks to the rapid development of artificial intelligence and machine learning technology, now we can use a lot of online transcription services to help us achieve our goals.</p><p>In this report, we are going to examine two factors speech speed, and accent, that could potentially affect the accuracy of audio transcription by measuring the word error rate(WER). We adopt [method] to generate 55 different texts by altering speech speed, and accent, and transcribe them to texts using Amazon Transcribe Service. We then calculate the corresponding word error rate for each speed and accent and analyze the results using a wide range of python statistics tools. Our assumption before data analysis is that first, when the speech speed increases, the accuracy will decrease holding other factors fixed, second, the accuracy of US accent will be higher than others. Based on the results, both accents and speed influence on Amazon transcribe accuracy, while accent poses a stronger effect than speed.</p><h2 id=method-overview>Method Overview<a hidden class=anchor aria-hidden=true href=#method-overview>#</a></h2><p>The core method in our project is <a href=https://aws.amazon.com/transcribe/>Amazon Transcribe</a> Services, which uses automatic speech recognition(ASR), a deep learning process, to convert speech to text quickly with high accuracy. The method enjoys four main benefits. First, it creates easily readable transcriptions by adding speaker diarization, punctuation and formatting. Second, it ensures customers privacy by identifying and redacting sensitive personally identifiable information and allowing contact centers to easily review and share the transcripts for the purpose of customer experience insight and agent training based on the PII. Third, it increases accuracy with customized transcriptions by allowing users to add new words to base vocabulary, such as product names, technical terminology, etc. or train the language model. Fourth, it filters specific words by enabling the user to mask or remove words that are unwanted.</p><h2 id=research-design>Research Design<a hidden class=anchor aria-hidden=true href=#research-design>#</a></h2><p><img loading=lazy src=/images/AWS/architecture_diagram.png alt=image1></p><p>We initially input SSML scripts to a Google text to audio converting service named Google TTS Audio Service, from which we generated two groups of audio files: ones with same speeds and different accents and ones with same accents and different speeds. To proceed, we uploaded them into two separate Amazon S3 buckets, and utilized Amazon Transcribe service to convert them to our source text files. Afterwards, we used the Jiwer speech evaluation recognition package to build the WER (world error rate) dataframe, where we compare the target original input SSML scripts and source text files we generated using Amazon Transcribe service. Finally, we input the WER dataframe to ANOVA analysis to testify whether our results are statistically significant.</p><h2 id=data-preparation>Data Preparation<a hidden class=anchor aria-hidden=true href=#data-preparation>#</a></h2><p>We generated raw audio files using an online voice-generator which applies the Google TTS Audio Service. Although Amazon has its own text-to-audio service named Amazon Polly, it has only four different accents available, which does not provide us enough sample data for the research. On the contrary Google TTS provides a variety of The content includes five different texts as followed, ranging from novels and website contents to news articles:</p><p>As the research aims to analyze the effect of accents and speed on the accuracy of text generated by Amazon Transcribe. The raw audios cover six different accents including French, Spanish, Japanese, Korean, UK, and US. The audio files were also generated at five different speeds including 0.5,0.8,1.0,1.2, and 1.5 in the US accent.</p><h2 id=transcribe-model-running>Transcribe Model Running<a hidden class=anchor aria-hidden=true href=#transcribe-model-running>#</a></h2><p>After downloading audio files from s3 bucket, we run the transcribe model using our generated audio file. We divided our audio files into two parts. In the first part, we choose files based on six different accents with five different versions of text. In the second parts, we choose files based on five different accents with five different versions of texts. We use five different versions of texts in order to avoid the bias caused by one certain type of text. We use loops to run the transcribe and extract data from the Transcribe model. The results are saved into the text lists which contain all converted versions of text after running the Transcribe model.</p><h2 id=word-error-rate-calculation>Word Error Rate Calculation<a hidden class=anchor aria-hidden=true href=#word-error-rate-calculation>#</a></h2><p>In order to evaluate the accuracy of transcribed text generated by Amazon Transcribe, we introduce a metric called Word Error Rate (WER). This method is recommended by the US National Institute of Standards and Technology for the evaluation of ASR systems.
WER is defined as the normalized Levenshtein edit distance. Levenshtein edit distance calculates the distance between the reference and the hypothesis.:</p><p>The formula is</p><p><img loading=lazy src=/images/AWS/wer_formula.png alt=image1></p><p>WER is the percentage of transcription errors produced by the ASR method compared to the number of words actually spoken. In other words, it’s the minimum number of words that need to be corrected to change the hypothesis transcript into the reference transcript, divided by the number of words that the speaker originally said.
According to this formula, we know that the lower the WER, the more accurate the transcribed text is, with 0 error rate the best. WER can go beyond 1 if there are too many insertion errors.</p><p>The usage of WER is simple. This algorithm is contained in the jiwer package. First we should install it using the pip function.
<img loading=lazy src=/images/AWS/install_jiwer.png alt=image1>
Then we import the wer function from the jiwer package
<img loading=lazy src=/images/AWS/import_jiwer.png alt=image1>
After we prepared the reference text and hypothesis text we wanted to test on, we can call the wer() function to get the result (origin refers to reference text, test refer to hypothesis text),
<img loading=lazy src=/images/AWS/wer_function.png alt=image1>
Calculating WER for all the transcribed text files we collected, we make our WER table and can move on to the statistical analysis part.
Below is a glimpse of the WER dataframe.
<img loading=lazy src=/images/AWS/all_wer_sample.png alt=image1></p><h2 id=data-visualization-and-analysis>Data Visualization And Analysis<a hidden class=anchor aria-hidden=true href=#data-visualization-and-analysis>#</a></h2><p>According to our OLS Regression and data visualization, accents and speed do affect the accuracy of Amazon Transcribe. Amazon Transcribe results in accuracies for different accents which rank from the highest to lowest as US, UK, Spanish, French, Korean, and Japanese. Meanwhile, based on the regression, the positive coefficient of scripts generated from Japanese and Korean accents and WER suggest the error rate for these two accents tends to be higher compared to other accents, whereas the negative coefficient of scripts generated from UK, Spanish, and US accents and WER suggests a more accurate transcription.</p><p>As for the speed, the WER values of different speeds do not show a significant difference given a large p-value. From boxplot visualization, we can find that the average value of WER in the five different speeds falls in the range around 0.15. We also find that the difference between speed 0.8 which has the largest WER and speed 1.0 which has lowest WER are not significant, which corresponds to our main finding. Based on regression model analysis, the coefficients are all negative for the categorical speeds. If treating speed as a numerical variable, for every unit increase in speed, the WER value will decrease.</p></div><footer class=post-footer><nav class=paginav><a class=prev href=https://horizonhou.github.io/projects/markov-chain/><span class=title>« Prev Page</span><br><span>Predicting Citi Bike Availability Using Markov Chains</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Researching the Impact of Multiple Factors in Speech Transcription Using Amazon Transcribe Service on twitter" href="https://twitter.com/intent/tweet/?text=Researching%20the%20Impact%20of%20Multiple%20Factors%20in%20Speech%20Transcription%20Using%20Amazon%20Transcribe%20Service&amp;url=https%3a%2f%2fhorizonhou.github.io%2fprojects%2faws-anova%2f&amp;hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Researching the Impact of Multiple Factors in Speech Transcription Using Amazon Transcribe Service on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fhorizonhou.github.io%2fprojects%2faws-anova%2f&amp;title=Researching%20the%20Impact%20of%20Multiple%20Factors%20in%20Speech%20Transcription%20Using%20Amazon%20Transcribe%20Service&amp;summary=Researching%20the%20Impact%20of%20Multiple%20Factors%20in%20Speech%20Transcription%20Using%20Amazon%20Transcribe%20Service&amp;source=https%3a%2f%2fhorizonhou.github.io%2fprojects%2faws-anova%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Researching the Impact of Multiple Factors in Speech Transcription Using Amazon Transcribe Service on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fhorizonhou.github.io%2fprojects%2faws-anova%2f&title=Researching%20the%20Impact%20of%20Multiple%20Factors%20in%20Speech%20Transcription%20Using%20Amazon%20Transcribe%20Service"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Researching the Impact of Multiple Factors in Speech Transcription Using Amazon Transcribe Service on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fhorizonhou.github.io%2fprojects%2faws-anova%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Researching the Impact of Multiple Factors in Speech Transcription Using Amazon Transcribe Service on whatsapp" href="https://api.whatsapp.com/send?text=Researching%20the%20Impact%20of%20Multiple%20Factors%20in%20Speech%20Transcription%20Using%20Amazon%20Transcribe%20Service%20-%20https%3a%2f%2fhorizonhou.github.io%2fprojects%2faws-anova%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Researching the Impact of Multiple Factors in Speech Transcription Using Amazon Transcribe Service on telegram" href="https://telegram.me/share/url?text=Researching%20the%20Impact%20of%20Multiple%20Factors%20in%20Speech%20Transcription%20Using%20Amazon%20Transcribe%20Service&amp;url=https%3a%2f%2fhorizonhou.github.io%2fprojects%2faws-anova%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://horizonhou.github.io/>Ruize Hou</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>